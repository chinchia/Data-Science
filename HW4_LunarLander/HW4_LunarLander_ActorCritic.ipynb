{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the env without installation and any error: \n",
    "- `conda create -n DS_HW4_env -c hcc -c conda-forge python=3.6 gym pybox2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, action_space, observation_space):\n",
    "        \n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.affine = nn.Linear(observation_space, 128)\n",
    "        self.action_layer = nn.Linear(128, action_space)\n",
    "        self.value_layer = nn.Linear(128, 1)\n",
    "        \n",
    "        self.log_probs = list()\n",
    "        self.state_values = list()\n",
    "        self.rewards = list()\n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        state = torch.from_numpy(state).float()\n",
    "        state = self.affine(state)\n",
    "        state = F.relu(state)\n",
    "        \n",
    "        state_value = self.value_layer(state)\n",
    "        \n",
    "        action_probs = self.action_layer(state)\n",
    "        action_probs = F.softmax(action_probs)\n",
    "        action_probs = Categorical(action_probs)\n",
    "        action = action_probs.sample()\n",
    "        \n",
    "        self.log_probs.append(action_probs.log_prob(action))\n",
    "        self.state_values.append(state_value)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def calculate_loss(self, gamma):\n",
    "        \n",
    "        rewards = list()\n",
    "        discounted_reward = 0\n",
    "        for reward in self.rewards[::-1]:\n",
    "            discounted_reward = reward + gamma * discounted_reward\n",
    "            rewards.insert(0, discounted_reward)\n",
    "                \n",
    "        # normalize rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std())\n",
    "        \n",
    "        loss = 0\n",
    "        for log_prob, value, reward in zip(self.log_probs, self.state_values, rewards):\n",
    "            advantage = reward - value.item()\n",
    "            action_loss = -log_prob * advantage\n",
    "            value_loss = F.smooth_l1_loss(value, reward)\n",
    "            loss = loss + action_loss + value_loss\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \n",
    "        del self.log_probs[:]\n",
    "        del self.state_values[:]\n",
    "        del self.rewards[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_trained):\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    policy = ActorCritic(action_space, observation_space)\n",
    "    policy.load_state_dict(torch.load(model_trained))\n",
    "\n",
    "    state = env.reset()\n",
    "    total_rewards = 0\n",
    "    for i in range(1000):\n",
    "        action = policy(state)\n",
    "        state, reward, finish, _ = env.step(action)\n",
    "        total_rewards += reward\n",
    "        env.render()\n",
    "        img = env.render(mode='rgb_array')\n",
    "        img = Image.fromarray(img)\n",
    "        img.save('./gif/{}.jpg'.format(i))\n",
    "            \n",
    "        if finish:\n",
    "            break\n",
    "                \n",
    "    print('reward: {}'.format(total_rewards))\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e1ecd61d10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set hyperparameters\n",
    "gamma = 0.99\n",
    "lr = 0.005\n",
    "random_seed = 999\n",
    "\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(random_seed)\n",
    "\n",
    "action_space = env.action_space.n\n",
    "observation_space = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== start training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\g1022\\anaconda3\\envs\\DS_HW4_env\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\g1022\\anaconda3\\envs\\DS_HW4_env\\lib\\site-packages\\ipykernel_launcher.py:49: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0\treward: -3.623541367590081\n",
      "episode 20\treward: -198.38652571980262\n",
      "episode 40\treward: -237.28594451889518\n",
      "episode 60\treward: -140.69476881770723\n",
      "episode 80\treward: -113.50247049259376\n",
      "episode 100\treward: -107.7351451199002\n",
      "episode 120\treward: -99.7934619501124\n",
      "episode 140\treward: -114.36156754171911\n",
      "episode 160\treward: -88.46699121784158\n",
      "episode 180\treward: -139.75554662908024\n",
      "episode 200\treward: -52.607110109417235\n",
      "episode 220\treward: -88.85572418767765\n",
      "episode 240\treward: -24.326902219236523\n",
      "episode 260\treward: -38.81775222323289\n",
      "episode 280\treward: -27.45102829376715\n",
      "episode 300\treward: -67.18412237415534\n",
      "episode 320\treward: 0.2179619036713259\n",
      "episode 340\treward: 25.956794690936572\n",
      "episode 360\treward: 6.098565557523329\n",
      "episode 380\treward: 10.667258308481093\n",
      "episode 400\treward: -4.8250284922650515\n",
      "episode 420\treward: 14.097604036575834\n",
      "episode 440\treward: 23.274690445391048\n",
      "episode 460\treward: -30.134386325358385\n",
      "episode 480\treward: -29.192558991778903\n",
      "episode 500\treward: 33.4537237123465\n",
      "episode 520\treward: 16.918172909045197\n",
      "episode 540\treward: 10.899917860350595\n",
      "episode 560\treward: 49.63929037468004\n",
      "episode 580\treward: 52.55992368142006\n",
      "episode 600\treward: 12.1292116147609\n",
      "episode 620\treward: 79.46509791358426\n",
      "episode 640\treward: 116.37189996065169\n",
      "episode 660\treward: 118.78369601127763\n",
      "episode 680\treward: 140.1341143240964\n",
      "episode 700\treward: 131.613216029058\n",
      "episode 720\treward: 113.66677654787752\n",
      "episode 740\treward: 109.56549466459407\n",
      "episode 760\treward: 98.49793303594339\n",
      "episode 780\treward: 33.212811819603374\n",
      "episode 800\treward: 106.13179138224254\n",
      "episode 820\treward: 144.02156196826667\n",
      "episode 840\treward: 94.26833551589267\n",
      "episode 860\treward: 121.46138263088838\n",
      "episode 880\treward: 113.18521627798293\n",
      "episode 900\treward: 110.29321808095067\n",
      "episode 920\treward: 109.11985429272522\n",
      "episode 940\treward: 123.35316965993738\n",
      "episode 960\treward: 146.4993686578315\n",
      "episode 980\treward: 129.60278557135177\n",
      "episode 1000\treward: 132.19160260110107\n",
      "episode 1020\treward: 141.92903882535114\n",
      "episode 1040\treward: 141.35830101306934\n",
      "episode 1060\treward: 111.42183125554057\n",
      "episode 1080\treward: 124.82121236791413\n",
      "episode 1100\treward: 122.74222051182585\n",
      "episode 1120\treward: 141.9028387240503\n",
      "episode 1140\treward: 141.76796561024918\n",
      "episode 1160\treward: 161.08277254204577\n",
      "episode 1180\treward: 117.77077818937873\n",
      "episode 1200\treward: 135.55846139362728\n",
      "episode 1220\treward: 142.11243278285824\n",
      "episode 1240\treward: 126.5596746695633\n",
      "episode 1260\treward: 129.28513957840045\n",
      "episode 1280\treward: 97.08049964127254\n",
      "episode 1300\treward: 130.14910315838202\n",
      "episode 1320\treward: 133.3322502593379\n",
      "episode 1340\treward: 129.7410942011631\n",
      "episode 1360\treward: 137.16854499052556\n",
      "episode 1380\treward: 127.23595245755489\n",
      "episode 1400\treward: 123.42381108845571\n",
      "episode 1420\treward: 142.41445635009572\n",
      "episode 1440\treward: 135.94779633414208\n",
      "episode 1460\treward: 131.33793209058675\n",
      "episode 1480\treward: 137.47552573432236\n",
      "episode 1500\treward: 140.6732442496687\n",
      "episode 1520\treward: 135.9036336817782\n",
      "episode 1540\treward: 148.80604038023202\n",
      "episode 1560\treward: 127.88077136700676\n",
      "episode 1580\treward: 141.0606834792986\n",
      "episode 1600\treward: 128.45024175767364\n",
      "episode 1620\treward: 151.5221378234578\n",
      "episode 1640\treward: 146.76578029003315\n",
      "episode 1660\treward: 124.5140611403958\n",
      "episode 1680\treward: 135.691832736957\n",
      "episode 1700\treward: 149.10916877587988\n",
      "episode 1720\treward: 150.1544702117979\n",
      "episode 1740\treward: 145.9116540996384\n",
      "episode 1760\treward: 139.19446909706363\n",
      "episode 1780\treward: 145.9038656673772\n",
      "episode 1800\treward: 141.88440175801196\n",
      "episode 1820\treward: 144.4427901496877\n",
      "episode 1840\treward: 133.47768432876083\n",
      "episode 1860\treward: 122.61759450944169\n",
      "episode 1880\treward: 144.72946489948885\n",
      "episode 1900\treward: 143.03423028608753\n",
      "episode 1920\treward: 145.20441178241927\n",
      "episode 1940\treward: 150.70342923865783\n",
      "episode 1960\treward: 146.92676032272738\n",
      "episode 1980\treward: 147.504078587253\n",
      "episode 2000\treward: 135.36068562846077\n",
      "episode 2020\treward: 156.22817395916974\n",
      "episode 2040\treward: 137.27877385909105\n",
      "episode 2060\treward: 105.61477196303574\n",
      "episode 2080\treward: 126.45284896739392\n",
      "episode 2100\treward: 151.33934453328445\n",
      "episode 2120\treward: 145.79812042506657\n",
      "episode 2140\treward: 135.72924390378952\n",
      "episode 2160\treward: 125.64076934490606\n",
      "episode 2180\treward: 144.08987784198564\n",
      "episode 2200\treward: 147.9001310384749\n",
      "episode 2220\treward: 146.3140328585219\n",
      "episode 2240\treward: 160.03431877350462\n",
      "episode 2260\treward: 133.22191630233925\n",
      "episode 2280\treward: 142.73650229384504\n",
      "episode 2300\treward: 174.5396743102341\n",
      "episode 2320\treward: 152.96843773964886\n",
      "episode 2340\treward: 142.35608519144074\n",
      "episode 2360\treward: 157.53192205420117\n",
      "episode 2380\treward: 161.2454745243987\n",
      "episode 2400\treward: 151.5470151363921\n",
      "episode 2420\treward: 158.26312234874842\n",
      "episode 2440\treward: 155.17725257669565\n",
      "episode 2460\treward: 157.95268561716634\n",
      "episode 2480\treward: 155.90486915819844\n",
      "episode 2500\treward: 148.56899774485504\n",
      "episode 2520\treward: 149.85074469522672\n",
      "episode 2540\treward: 148.70329382197806\n",
      "episode 2560\treward: 149.7148097407732\n",
      "episode 2580\treward: 152.98755277330378\n",
      "episode 2600\treward: 149.4936941784695\n",
      "episode 2620\treward: 147.3874898868448\n",
      "episode 2640\treward: 149.98934757747344\n",
      "episode 2660\treward: 164.93571390115795\n",
      "episode 2680\treward: 142.49999340758364\n",
      "episode 2700\treward: 137.36055810796796\n",
      "episode 2720\treward: 148.03568347880773\n",
      "episode 2740\treward: 146.49438593920905\n",
      "episode 2760\treward: 152.60405289047975\n",
      "episode 2780\treward: 147.43037241121772\n",
      "episode 2800\treward: 152.64600845867085\n",
      "episode 2820\treward: 141.46694785114826\n",
      "episode 2840\treward: 152.75726343138732\n",
      "episode 2860\treward: 154.7276895555682\n",
      "episode 2880\treward: 148.62501936538072\n",
      "episode 2900\treward: 160.09021778881987\n",
      "episode 2920\treward: 147.66001374512234\n",
      "episode 2940\treward: 153.21016410383262\n",
      "episode 2960\treward: 153.94093269713682\n",
      "episode 2980\treward: 163.8182767098271\n",
      "episode 3000\treward: 164.9985383252323\n",
      "episode 3020\treward: 157.7457828432413\n",
      "episode 3040\treward: 159.25115244652886\n",
      "episode 3060\treward: 155.4547960447211\n",
      "episode 3080\treward: 142.33175041614413\n",
      "episode 3100\treward: 153.48390000024682\n",
      "episode 3120\treward: 151.39018285153065\n",
      "episode 3140\treward: 167.2875044771729\n",
      "episode 3160\treward: 166.20978173178202\n",
      "episode 3180\treward: 166.28822294258956\n",
      "episode 3200\treward: 157.86959963189247\n",
      "episode 3220\treward: 156.28031519719715\n",
      "episode 3240\treward: 159.10344015082714\n",
      "episode 3260\treward: 164.5678590583791\n",
      "episode 3280\treward: 171.03667342728008\n",
      "episode 3300\treward: 218.77851946686368\n",
      "episode 3320\treward: 218.066328170164\n",
      "episode 3340\treward: 162.28446957704898\n",
      "episode 3360\treward: 59.818969617249174\n",
      "episode 3380\treward: 114.95097621521305\n",
      "episode 3400\treward: 140.83303455032228\n",
      "episode 3420\treward: 166.2456015986607\n",
      "episode 3440\treward: 217.55211755344686\n",
      "episode 3460\treward: 218.5648382961909\n",
      "episode 3480\treward: 217.15792171530202\n",
      "episode 3500\treward: 198.97808293526631\n",
      "episode 3520\treward: 240.40287980252816\n",
      "episode 3540\treward: 229.1016422413176\n",
      "episode 3560\treward: 157.61700262806025\n",
      "episode 3580\treward: 206.72293060633055\n",
      "episode 3600\treward: 228.45971897096402\n",
      "episode 3620\treward: 237.29003974204267\n",
      "=== start testing ===\n",
      "reward: 267.9394614613904\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "policy = ActorCritic(action_space, observation_space)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "print('=== start training ===')\n",
    "\n",
    "total_rewards = 0\n",
    "for episode in range(5000):\n",
    "    state = env.reset() # first observation of this episode\n",
    "    \n",
    "    for i in range(1000):\n",
    "        action = policy(state) # choose action\n",
    "        state, reward, finish, _ = env.step(action) # get next state\n",
    "        policy.rewards.append(reward)\n",
    "        total_rewards += reward\n",
    "        \n",
    "        if finish:\n",
    "            break\n",
    "                    \n",
    "    # update policy\n",
    "    optimizer.zero_grad()\n",
    "    loss = policy.calculate_loss(gamma)\n",
    "    loss.backward()\n",
    "    optimizer.step()        \n",
    "    policy.clear_memory()\n",
    "        \n",
    "    # save the model and output result images if average reward > 250\n",
    "    if total_rewards > 5000:\n",
    "        torch.save(policy.state_dict(), 'model.pth')\n",
    "        print('=== start testing ===')\n",
    "        test('model.pth')\n",
    "        break\n",
    "        \n",
    "    if episode % 20 == 0:\n",
    "        total_rewards /= 20\n",
    "        print('episode {}\\treward: {}'.format(episode, total_rewards))\n",
    "        \n",
    "        total_rewards = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
